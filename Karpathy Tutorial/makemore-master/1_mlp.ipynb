{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary of characters and mappings to/from indexeger index\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "char_to_index = {s:i+1 for i,s in enumerate(chars)}\n",
    "char_to_index['.'] = 0\n",
    "index_to_char = {i:s for s,i in char_to_index.items()}\n",
    "index_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data_set\n",
    "block_size = 3 # how many chars we view to predict the next one\n",
    "_input, _label= [],[]\n",
    "\n",
    "for w in words:\n",
    "    # print(w)\n",
    "    context =  [0]*block_size\n",
    "    for ch in w+'.':\n",
    "        label_index = char_to_index[ch]\n",
    "        _input.append(context)\n",
    "        _label.append(label_index)\n",
    "        #print(''.join(index_to_char[i] for i in context), ' --->', ch)\n",
    "        context = context[1:]+[label_index]\n",
    "        \n",
    "inputs = torch.tensor(_input)\n",
    "labels = torch.tensor(_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) \n",
    "\n",
    "C = torch.randn((27,2), generator=g) # 27 char, 2 dim per char embedding (feature space)\n",
    "\n",
    "\n",
    "\n",
    "W1 = torch.randn((6,100) , generator=g) # 3 chars, each in 2 dim\n",
    "b1 = torch.randn(100, generator=g) # we want to do emb @ W1 + b1\n",
    "\n",
    "\n",
    "W2 = torch.randn((100,27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "\n",
    "parameters = [C, W1, W2, b1, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26486366987228394\n",
      "0.264577716588974\n",
      "0.2647497057914734\n",
      "0.2644633650779724\n",
      "0.26462528109550476\n",
      "0.2643424868583679\n",
      "0.2644961476325989\n",
      "0.26421913504600525\n",
      "0.26436570286750793\n",
      "0.2640955448150635\n"
     ]
    }
   ],
   "source": [
    "for _ in range (10):\n",
    "    '''Forward Pass'''\n",
    "    emb = C[inputs] # embed the whole inputs into features\n",
    "\n",
    "    hidden = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
    "\n",
    "    logits = hidden @ W2 + b2 # shape 32, 27\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    print(loss.item())\n",
    "    '''Backward Pass'''\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
