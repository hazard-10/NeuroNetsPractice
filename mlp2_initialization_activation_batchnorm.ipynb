{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 0. Check cuda availability\n",
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda:0\" \n",
    "else: \n",
    "    dev = \"cpu\" \n",
    "device = torch.device(dev) \n",
    "\n",
    "# 1. Load the data from local file called 'names.txt'\n",
    "with open('names.txt', 'r') as f:\n",
    "    names = [line.strip() for line in f]\n",
    "len(names), names[:5]\n",
    "\n",
    "# 2. encode the char into a list of integers\n",
    "symbols = sorted(list(set(''.join(names))))\n",
    "char_to_int = {s:i+1 for i,s in enumerate(symbols)}\n",
    "int_to_char = {i+1:s for i,s in enumerate(symbols)}\n",
    "char_to_int['.'] = 0\n",
    "int_to_char[0] = '.'\n",
    "# char_to_int, int_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "block_size = 3\n",
    "embedding_size = 3\n",
    "hidden_size = 200\n",
    "minibatch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([159609, 3]),\n",
       " torch.Size([45723, 3]),\n",
       " torch.Size([22814, 3]),\n",
       " torch.Size([159609]),\n",
       " torch.Size([45723]),\n",
       " torch.Size([22814]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. generate mapping from previous characters to next character\n",
    "def build_dataset(names_set):\n",
    "    input_char, output_char = [],[]\n",
    "    for word in names_set[:]:\n",
    "        input_word ='.'*block_size+word\n",
    "        output_word = word + '.'\n",
    "        for i in range(len(output_word)):\n",
    "            input_char.append(list(input_word[i:i+block_size])) \n",
    "            output_char.append(output_word[i]) \n",
    "            \n",
    "    # encode mapping into integers\n",
    "    for i in range(len(input_char)):\n",
    "        input_char[i] = [char_to_int[s] for s in input_char[i]]\n",
    "        output_char[i] = char_to_int[output_char[i]]\n",
    "    X = torch.tensor(input_char, device=device)\n",
    "    Y = torch.tensor(output_char, device=device)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# split dataset into train, validation, and testing\n",
    "import random\n",
    "random.shuffle(names)\n",
    "train_size = int(len(names)*0.7)\n",
    "val_size = int(len(names)*0.2)\n",
    "Inputs_train, Labels_train = build_dataset( names[:train_size])\n",
    "Inputs_val, Labels_val = build_dataset( names[train_size:train_size+val_size])\n",
    "Inputs_test, Labels_test = build_dataset( names[train_size+val_size:])\n",
    "Inputs_train.shape, Inputs_val.shape, Inputs_test.shape, Labels_train.shape, Labels_val.shape, Labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 3]),\n",
       " torch.Size([9, 200]),\n",
       " torch.Size([200]),\n",
       " torch.Size([200, 27]),\n",
       " torch.Size([27]),\n",
       " 7508)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the parameters\n",
    "\n",
    "# embedding layer\n",
    "E = torch.rand((len(char_to_int), embedding_size), device=device, requires_grad=True)\n",
    "\n",
    "# Tanh Hidden layer\n",
    "W_hidden = torch.rand((embedding_size*block_size, hidden_size), device=device, requires_grad=True) *0.5\n",
    "b_hidden = torch.rand(hidden_size, device=device, requires_grad=True) * 0.1\n",
    "\n",
    "# softmax output layer\n",
    "W_out = torch.rand((hidden_size, len(char_to_int)), device=device, requires_grad=True)*0.1\n",
    "b_out = torch.rand( len(char_to_int), device=device, requires_grad=True)*0.1\n",
    "\n",
    "params = [E, W_hidden, b_hidden, W_out, b_out]\n",
    "\n",
    "E.shape,W_hidden.shape, b_hidden.shape, W_out.shape, b_out.shape,sum([p.numel() for p in params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/200000. loss: 3.2984704971313477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_313/2404655033.py:23: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
      "  p.data += -1 * learning_rate * p.grad\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39m100000\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0.01\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m params:\n\u001b[0;32m---> 23\u001b[0m     p\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m*\u001b[39;49m learning_rate \u001b[39m*\u001b[39;49m p\u001b[39m.\u001b[39;49mgrad\n\u001b[1;32m     25\u001b[0m loss_records\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "loss_records = []\n",
    "for i in range(max_steps):\n",
    "    # construct minibatch\n",
    "    index_this_batch = torch.randint(0, len(Inputs_train), (minibatch_size,))\n",
    "    \n",
    "    # forward pass\n",
    "    embed = E[Inputs_train[index_this_batch]]\n",
    "    hid = torch.tanh(embed.view(-1, embedding_size*block_size) @ W_hidden + b_hidden)\n",
    "    log_counts = hid @ W_out + b_out\n",
    "    loss = F.cross_entropy(log_counts, Labels_train[index_this_batch])\n",
    "    if (i % 10000 == 0):\n",
    "        print(str(i)+\"/200000. loss: \"+str(loss.item()))\n",
    "    \n",
    "    # backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # gradient descent\n",
    "    learning_rate = 0.1 if i < 100000 else 0.01\n",
    "    for p in params:\n",
    "        p.data += -1 * learning_rate * p.grad\n",
    "    \n",
    "    loss_records.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_records, label='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_loss(split):\n",
    "    x,y = {\n",
    "        \"train\": (Inputs_train, Labels_train),\n",
    "        \"val\": (Inputs_val, Labels_val),\n",
    "        \"test\": (Inputs_test, Labels_test)\n",
    "    }[split]\n",
    "\n",
    "    embed = E[x]\n",
    "    hid = torch.tanh(embed.view(-1, embedding_size*block_size) @ W_hidden + b_hidden)\n",
    "    log_counts = hid @ W_out + b_out\n",
    "    loss = F.cross_entropy(log_counts, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "eval_loss(\"train\")\n",
    "eval_loss(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1000, 10)\n",
    "weights = torch.randn(10, 200)\n",
    "\n",
    "output = input @ weights\n",
    "print(input.mean(), input.std())\n",
    "print(output.mean(), output.std())\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(121)\n",
    "plt.hist(input.view(-1).tolist(), 50, density=True)\n",
    "plt.subplot(122)\n",
    "plt.hist(output.view(-1).tolist(), 50, density=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
