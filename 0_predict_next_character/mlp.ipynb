{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32033, ['emma', 'olivia', 'ava', 'isabella', 'sophia'])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 0. Check cuda availability\n",
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda:0\" \n",
    "else: \n",
    "    dev = \"cpu\" \n",
    "device = torch.device(dev) \n",
    "\n",
    "# 1. Load the data from local file called 'names.txt'\n",
    "with open('names.txt', 'r') as f:\n",
    "    names = [line.strip() for line in f]\n",
    "len(names), names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. encode the char into a list of integers\n",
    "symbols = sorted(list(set(''.join(names))))\n",
    "char_to_int = {s:i+1 for i,s in enumerate(symbols)}\n",
    "int_to_char = {i+1:s for i,s in enumerate(symbols)}\n",
    "char_to_int['.'] = 0\n",
    "int_to_char[0] = '.'\n",
    "# char_to_int\n",
    "# int_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "block_size = 3\n",
    "embedding_size = 30\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. generate mapping from previous characters to next character\n",
    "input_char, output_char = [],[]\n",
    "for word in names[:]:\n",
    "    input_word ='.'*block_size+word\n",
    "    output_word = word + '.'\n",
    "    for i in range(len(output_word)):\n",
    "        input_char.append(list(input_word[i:i+block_size])) \n",
    "        output_char.append(output_word[i]) \n",
    "# input_char, output_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode mapping into integers\n",
    "for i in range(len(input_char)):\n",
    "    input_char[i] = [char_to_int[s] for s in input_char[i]]\n",
    "    output_char[i] = char_to_int[output_char[i]]\n",
    "input_char = torch.tensor(input_char, device=device)\n",
    "labels = torch.tensor(output_char, device=device)\n",
    "# input_char.shape, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([684, 3, 27]),\n",
       " device(type='cuda', index=0),\n",
       " torch.Size([684]),\n",
       " device(type='cuda', index=0))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding\n",
    "\n",
    "input_encoded = F.one_hot(input_char, len(char_to_int)).float()\n",
    "input_encoded.shape, input_encoded.device, labels.shape, labels.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 30]),\n",
       " torch.Size([90, 100]),\n",
       " torch.Size([1, 100]),\n",
       " torch.Size([100, 27]),\n",
       " torch.Size([1, 27]),\n",
       " 12637)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding layer\n",
    "E = torch.rand((len(char_to_int), embedding_size), device=device, requires_grad=True)\n",
    "\n",
    "# Hidden layer\n",
    "W_hidden = torch.rand((embedding_size*block_size, hidden_size), device=device, requires_grad=True)\n",
    "b_hidden = torch.rand((1, hidden_size), device=device, requires_grad=True)\n",
    "\n",
    "# Output layer\n",
    "W_out = torch.rand((hidden_size, len(char_to_int)), device=device, requires_grad=True)\n",
    "b_out = torch.rand((1, len(char_to_int)), device=device, requires_grad=True)\n",
    "\n",
    "params = [E, W_hidden, b_hidden, W_out, b_out]\n",
    "\n",
    "E.shape,W_hidden.shape, b_hidden.shape, W_out.shape, b_out.shape,sum([p.numel() for p in params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.081396102905273\n",
      "4.944773197174072\n",
      "4.822131633758545\n",
      "4.710469722747803\n",
      "4.607940673828125\n",
      "4.513275623321533\n",
      "4.425487518310547\n",
      "4.343755722045898\n",
      "4.267368316650391\n",
      "4.195699691772461\n",
      "4.128204822540283\n",
      "4.064415454864502\n",
      "4.003929615020752\n",
      "3.9464218616485596\n",
      "3.8916237354278564\n",
      "3.8393261432647705\n",
      "3.789365291595459\n",
      "3.7416231632232666\n",
      "3.6960175037384033\n",
      "3.6524875164031982\n",
      "3.611006259918213\n",
      "3.5715625286102295\n",
      "3.534148693084717\n",
      "3.4987692832946777\n",
      "3.465425968170166\n",
      "3.434112071990967\n",
      "3.4048047065734863\n",
      "3.377467155456543\n",
      "3.3520376682281494\n",
      "3.3284311294555664\n",
      "3.306548595428467\n",
      "3.2862627506256104\n",
      "3.2674410343170166\n",
      "3.249941349029541\n",
      "3.2336225509643555\n",
      "3.2183423042297363\n",
      "3.203975200653076\n",
      "3.190399408340454\n",
      "3.17751407623291\n",
      "3.1652274131774902\n",
      "3.1534616947174072\n",
      "3.142157554626465\n",
      "3.1312623023986816\n",
      "3.120734930038452\n",
      "3.110539674758911\n",
      "3.1006557941436768\n",
      "3.0910584926605225\n",
      "3.081735372543335\n",
      "3.0726726055145264\n",
      "3.0638599395751953\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    '''forward pass'''\n",
    "    embed = E[input_char]\n",
    "    hid = torch.tanh(embed.view(-1, embedding_size*block_size) @ W_hidden + b_hidden)\n",
    "    log_counts = hid @ W_out + b_out\n",
    "    loss = F.cross_entropy(log_counts, labels)\n",
    "    print(loss.item())\n",
    "    '''backward pass'''\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in params:\n",
    "        p.data -= .01 * p.grad\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yi.\n",
      "s.\n",
      "l.\n",
      "ialoi.\n",
      "eygaea.\n",
      "lz.\n",
      "vlelnsyala.\n",
      "legibeillu.\n",
      "aeoaidoaahnqesaaee.\n",
      "hyeieeievolvsslaa.\n"
     ]
    }
   ],
   "source": [
    "# generate a name\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    context = [0]*block_size\n",
    "    while True:\n",
    "        embed = E[torch.tensor(context, device=device)]\n",
    "        hid = torch.tanh(embed.view(-1, embedding_size*block_size) @ W_hidden + b_hidden)\n",
    "        log_counts = hid @ W_out + b_out\n",
    "        probs = F.softmax(log_counts, dim=1)\n",
    "        index = torch.multinomial(probs, 1).item()\n",
    "        context = context[1:] + [index]\n",
    "        out.append(index)\n",
    "        if index == 0:\n",
    "            break\n",
    "    print(''.join([int_to_char[i] for i in out]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
